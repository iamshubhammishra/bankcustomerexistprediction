{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e2dd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 10000 stored elements and shape (10000, 3)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "import pickle\n",
    "\n",
    "\n",
    "## load the dataset\n",
    "data= pd.read_csv(\"Churn_Modelling.csv\")\n",
    "data.head()\n",
    "\n",
    "## Preprocess the data\n",
    "### Drop irrelevant column\n",
    "data=data.drop(['RowNumber','CustomerId','Surname'],axis=1)\n",
    "data\n",
    "\n",
    "## encode categorical variables\n",
    "'''\n",
    "ğŸ‘‰ fit()\n",
    "\n",
    "Reads unique values and assigns a number:\n",
    "\n",
    "\"Female\" â†’ 0\n",
    "\n",
    "\"Male\" â†’ 1\n",
    "(Alphabetical order: Female comes before Male)\n",
    "\n",
    "ğŸ‘‰ transform()\n",
    "\n",
    "Replaces text with numbers in the column.\n",
    "'''\n",
    "label_encoder_gender=LabelEncoder()\n",
    "data['Gender']=label_encoder_gender.fit_transform(data['Gender'])\n",
    "\n",
    "## Onehot encode 'geogrphy column'\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder_geo = OneHotEncoder()\n",
    "geo_encoder = one_hot_encoder_geo.fit_transform(data[['Geography']])\n",
    "geo_encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97444ada",
   "metadata": {},
   "source": [
    "ğŸ¯ What OneHotEncoder does\n",
    "\n",
    "Suppose your column has:\n",
    "\n",
    "Geography\n",
    "France\n",
    "Spain\n",
    "Germany\n",
    "\n",
    "OneHotEncoder converts this into:\n",
    "\n",
    "France\tSpain\tGermany\n",
    "1\t    0\t    0\n",
    "0\t    1\t    0\n",
    "0\t    0\t    1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ec2901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Geography_France', 'Geography_Germany', 'Geography_Spain'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder_geo.get_feature_names_out(['Geography'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0905aa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Geography_France  Geography_Germany  Geography_Spain\n",
       "0                  1.0                0.0              0.0\n",
       "1                  0.0                0.0              1.0\n",
       "2                  1.0                0.0              0.0\n",
       "3                  1.0                0.0              0.0\n",
       "4                  0.0                0.0              1.0\n",
       "...                ...                ...              ...\n",
       "9995               1.0                0.0              0.0\n",
       "9996               1.0                0.0              0.0\n",
       "9997               1.0                0.0              0.0\n",
       "9998               0.0                1.0              0.0\n",
       "9999               1.0                0.0              0.0\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_encoded_df= pd.DataFrame(geo_encoder.toarray(),columns=one_hot_encoder_geo.get_feature_names_out(['Geography']))\n",
    "geo_encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "598ba204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore  Gender  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
       "0          619       0   42       2       0.00              1          1   \n",
       "1          608       0   41       1   83807.86              1          0   \n",
       "2          502       0   42       8  159660.80              3          1   \n",
       "3          699       0   39       1       0.00              2          0   \n",
       "4          850       0   43       2  125510.82              1          1   \n",
       "\n",
       "   IsActiveMember  EstimatedSalary  Exited  Geography_France  \\\n",
       "0               1        101348.88       1               1.0   \n",
       "1               1        112542.58       0               0.0   \n",
       "2               0        113931.57       1               1.0   \n",
       "3               0         93826.63       0               1.0   \n",
       "4               1         79084.10       0               0.0   \n",
       "\n",
       "   Geography_Germany  Geography_Spain  \n",
       "0                0.0              0.0  \n",
       "1                0.0              1.0  \n",
       "2                0.0              0.0  \n",
       "3                0.0              0.0  \n",
       "4                0.0              1.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## combine one hot encoder with original data\n",
    "data= pd.concat([data.drop('Geography',axis=1),geo_encoded_df],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81cc4baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the conders and sscaler\n",
    "with open('label_encoder_gender.pkl', 'wb') as file:\n",
    "    pickle.dump(label_encoder_gender,file)\n",
    "with open('onehot_encoder_geo.pkl', 'wb') as file:\n",
    "    pickle.dump(one_hot_encoder_geo,file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4014a2e",
   "metadata": {},
   "source": [
    "ğŸ§  What this does\n",
    "\n",
    "You are saving your trained encoders to disk using pickle.\n",
    "\n",
    "This is very important when you build ML models for deploymentâ€”\n",
    "because when new data comes, you must encode it exactly the same way as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cbd52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35649971,  0.91324755, -0.6557859 , ...,  1.00150113,\n",
       "        -0.57946723, -0.57638802],\n",
       "       [-0.20389777,  0.91324755,  0.29493847, ..., -0.99850112,\n",
       "         1.72572313, -0.57638802],\n",
       "       [-0.96147213,  0.91324755, -1.41636539, ..., -0.99850112,\n",
       "        -0.57946723,  1.73494238],\n",
       "       ...,\n",
       "       [ 0.86500853, -1.09499335, -0.08535128, ...,  1.00150113,\n",
       "        -0.57946723, -0.57638802],\n",
       "       [ 0.15932282,  0.91324755,  0.3900109 , ...,  1.00150113,\n",
       "        -0.57946723, -0.57638802],\n",
       "       [ 0.47065475,  0.91324755,  1.15059039, ..., -0.99850112,\n",
       "         1.72572313, -0.57638802]], shape=(8000, 12))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "âœ… What this code does\n",
    "\n",
    "You are splitting your dataset into:\n",
    "\n",
    "X â†’ features (input variables)\n",
    "\n",
    "y â†’ target label (output variable)\n",
    "'''\n",
    "\n",
    "## Divide the dataset in to independent and dependent feature\n",
    "X = data.drop('Exited', axis=1)\n",
    "y = data['Exited']\n",
    "\n",
    "''''\n",
    "âœ… Why do we split the data?\n",
    "\n",
    "Because:\n",
    "\n",
    "Training data â†’ used to train the machine learning model\n",
    "\n",
    "Test data â†’ used to check how well the model performs on unseen data\n",
    "\n",
    "Without splitting, you cannot measure accuracy properly.\n",
    "\n",
    "test_size = 0.2\n",
    "Means:\n",
    "\n",
    "80% â†’ training data\n",
    "\n",
    "20% â†’ testing data\n",
    "\n",
    "If you want 30% test data, you would use test_size=0.3.\n",
    "\n",
    "random_state=42\n",
    "This ensures the same random split every time.\n",
    "\n",
    "ğŸ‘‰ universal, funny default seed for randomness\n",
    "\n",
    "You can use any number:\n",
    "\n",
    "random_state=1\n",
    "\n",
    "random_state=99\n",
    "\n",
    "random_state=2024\n",
    "\n",
    "It doesnâ€™t matter.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "## split the data in training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "'''\n",
    "z = x-mean / std\n",
    "\n",
    "âœ” fit() reads training data\n",
    "â€ƒâ€ƒâ†’ finds mean of each column\n",
    "â€ƒâ€ƒâ†’ finds standard deviation of each column\n",
    "\n",
    "âœ” transform() then scales the training data\n",
    "'''\n",
    "## scale these features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22856c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scaler.pkl','wb') as file:\n",
    "    pickle.dump(scaler,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863ce70",
   "metadata": {},
   "source": [
    "ANN Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb703e",
   "metadata": {},
   "source": [
    "This code builds a 3-layer Artificial Neural Network (ANN) using TensorFlow/Keras.\n",
    "Input â†’ HL1 â†’ HL2 â†’ Output\n",
    "\n",
    "ğŸ”¶ Layer 1: Dense(64, activation='relu', input_shape=(X_train.shape[1],))\n",
    "âœ” Dense Layer\n",
    "\n",
    "Fully connected layer.\n",
    "\n",
    "âœ” Units = 64\n",
    "\n",
    "This means 64 neurons in this hidden layer.\n",
    "\n",
    "Each neuron learns different patterns/features.\n",
    "\n",
    "âœ” activation='relu'\n",
    "\n",
    "ReLU gives non-linearity:\n",
    "\n",
    "ReLU(x) = max(0, x)\n",
    "\n",
    "\n",
    "It helps the network learn complex patterns.\n",
    "\n",
    "âœ” input_shape=(X_train.shape[1],)\n",
    "\n",
    "The network expects as many inputs as there are columns in X_train.\n",
    "\n",
    "Example: If you have 10 features â†’ input_shape = (10,)\n",
    "\n",
    "This layer is the bridge from input layer to HL1.\n",
    "\n",
    "\n",
    "ğŸ”¶ Layer 2: Dense(32, activation='relu')\n",
    "âœ” 32 neurons\n",
    "\n",
    "It reduces (compresses) features learned from Layer 1.\n",
    "\n",
    "âœ” activation='relu'\n",
    "\n",
    "Again helps in learning non-linear relationships.\n",
    "\n",
    "This is your second hidden layer.\n",
    "\n",
    "ğŸ”¶ Output Layer: Dense(1, activation='sigmoid')\n",
    "âœ” Units = 1\n",
    "\n",
    "You want one output â†’ probability of class 1.\n",
    "\n",
    "Examples:\n",
    "\n",
    "churn probability\n",
    "\n",
    "survived or not\n",
    "\n",
    "will exit or not\n",
    "\n",
    "spam or not\n",
    "\n",
    "âœ” activation='sigmoid'\n",
    "output = 0 to 1\n",
    "\n",
    "\n",
    "Sigmoid converts raw output into probability.\n",
    "\n",
    "X_train â†’ rows = samples  \n",
    "        â†’ columns = features\n",
    "        If your dataset has:\n",
    "\n",
    "800 rows (samples)\n",
    "\n",
    "10 columns (features)\n",
    "\n",
    "Then:\n",
    "\n",
    "X_train.shape = (800, 10)\n",
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "800 = number of training samples\n",
    "\n",
    "10 = number of input features\n",
    "\n",
    "ğŸ”¥ Why write (X_train.shape[1],) instead of just X_train.shape[1]?\n",
    "\n",
    "Because:\n",
    "\n",
    "Keras expects the input shape to be a tuple\n",
    "\n",
    "Even if there is only one dimension, it must be written like:\n",
    "\n",
    "(10,)   âœ… correct\n",
    "10      âŒ incorrect\n",
    "\n",
    "\n",
    "This defines the shape of one input sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e56e40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import datetime\n",
    "\n",
    "## Build Our ANN Model\n",
    "model = Sequential([\n",
    "    Dense(64,activation='relu', input_shape=(X_train.shape[1],)), ## HL1 connected with input layer\n",
    "    Dense(32, activation='relu'),   ## HL2\n",
    "    Dense(1, activation='sigmoid') ## Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af669b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚           \u001b[38;5;34m832\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_10 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_11 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,945</span> (11.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,945\u001b[0m (11.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,945</span> (11.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,945\u001b[0m (11.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011656a",
   "metadata": {},
   "source": [
    "ğŸ§  Why create optimizer manually?\n",
    "\n",
    "You get full control:\n",
    "\n",
    "change learning rate\n",
    "\n",
    "add decay\n",
    "\n",
    "add momentum (for SGD)\n",
    "\n",
    "customize optimization settings\n",
    "\n",
    "Example:\n",
    "\n",
    "opt = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "âœ… 2. Creating Loss Function\n",
    "losses = tensorflow.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "âœ” What does this do?\n",
    "\n",
    "Binary Crossentropy is the perfect loss for:\n",
    "\n",
    "sigmoid output\n",
    "\n",
    "binary classification (0/1)\n",
    "\n",
    "It measures how far prediction is from true label.\n",
    "\n",
    "Exactly same as writing:\n",
    "\n",
    "âœ… 3. Compiling Model\n",
    "model.compile(optimizer=opt, loss=losses, metrics=['accuracy'])\n",
    "\n",
    "âœ¨ Full Simple Explanation\n",
    "\n",
    "This code means:\n",
    "\n",
    "â€œTrain my model using Adam optimizer with learning rate 0.01.\n",
    "Evaluate error using Binary Crossentropy.\n",
    "Show accuracy while training.â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53ebcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compile modal\n",
    "import tensorflow\n",
    "opt = tensorflow.keras.optimizers.Adam(learning_rate=0.01)\n",
    "losses = tensorflow.keras.losses.BinaryCrossentropy()\n",
    "model.compile(optimizer=opt, loss=losses, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccac30a",
   "metadata": {},
   "source": [
    "This code is used to enable TensorBoard, which is a tool to visualize:\n",
    "\n",
    "loss\n",
    "\n",
    "accuracy\n",
    "\n",
    "learning curves\n",
    "\n",
    "histograms\n",
    "\n",
    "model graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c01f6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up the tensorboard\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "log_dir = \"logs/fit\"+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorflow_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592eef4",
   "metadata": {},
   "source": [
    "â­ 1ï¸âƒ£ What is Early Stopping?\n",
    "\n",
    "Early stopping automatically stops training when the model stops improving.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because if you keep training too long:\n",
    "\n",
    "the model will start overfitting\n",
    "\n",
    "validation loss will increase\n",
    "\n",
    "accuracy will drop on unseen data\n",
    "\n",
    "Early stopping prevents this.\n",
    "\n",
    "ğŸ”¶ monitor='val_loss'\n",
    "\n",
    "This tells the callback:\n",
    "\n",
    "â€œWatch the validation loss during training.â€\n",
    "\n",
    "Validation loss is the best indicator of generalization.\n",
    "\n",
    "If validation loss stops improving â†’ model is learning noise.\n",
    "\n",
    "ğŸ”¶ patience=5\n",
    "\n",
    "Meaning:\n",
    "\n",
    "â€œWait for 5 epochs even if validation loss is not improving.â€\n",
    "\n",
    "ğŸ”¶ restore_best_weights=True\n",
    "\n",
    "This is very important.\n",
    "\n",
    "When training runs:\n",
    "\n",
    "weights keep changing every epoch\n",
    "\n",
    "the best epoch (lowest val_loss) may not be the last epoch\n",
    "\n",
    "With this option:\n",
    "\n",
    "After stopping, the model automatically loads the best weights it ever had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6f3355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up early stopping\n",
    "early_stoping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f7c9cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8586 - loss: 0.3456 - val_accuracy: 0.8550 - val_loss: 0.3574\n",
      "Epoch 2/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8636 - loss: 0.3380 - val_accuracy: 0.8520 - val_loss: 0.3512\n",
      "Epoch 3/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8593 - loss: 0.3398 - val_accuracy: 0.8655 - val_loss: 0.3477\n",
      "Epoch 4/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8646 - loss: 0.3378 - val_accuracy: 0.8590 - val_loss: 0.3423\n",
      "Epoch 5/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8630 - loss: 0.3324 - val_accuracy: 0.8595 - val_loss: 0.3476\n",
      "Epoch 6/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8635 - loss: 0.3336 - val_accuracy: 0.8600 - val_loss: 0.3507\n",
      "Epoch 7/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8650 - loss: 0.3295 - val_accuracy: 0.8595 - val_loss: 0.3478\n",
      "Epoch 8/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8649 - loss: 0.3307 - val_accuracy: 0.8580 - val_loss: 0.3507\n",
      "Epoch 9/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8639 - loss: 0.3281 - val_accuracy: 0.8620 - val_loss: 0.3449\n",
      "Epoch 10/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8652 - loss: 0.3229 - val_accuracy: 0.8585 - val_loss: 0.3470\n",
      "Epoch 11/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8660 - loss: 0.3222 - val_accuracy: 0.8555 - val_loss: 0.3530\n",
      "Epoch 12/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8684 - loss: 0.3206 - val_accuracy: 0.8555 - val_loss: 0.3613\n",
      "Epoch 13/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8673 - loss: 0.3196 - val_accuracy: 0.8600 - val_loss: 0.3570\n",
      "Epoch 14/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8705 - loss: 0.3159 - val_accuracy: 0.8590 - val_loss: 0.3586\n"
     ]
    }
   ],
   "source": [
    "## Training modal\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=100,\n",
    "                    callbacks = [tensorflow_callback, early_stoping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff6d7ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b70e5ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "## Load tensorboard extension\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2bfda075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 6468), started 0:00:05 ago. (Use '!kill 6468' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-93848a2102bd3dd8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-93848a2102bd3dd8\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit20251118-222919"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
